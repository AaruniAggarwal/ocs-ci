tests:
   - test:
       name: install ceph pre-requisities
       module: install_prereq.py
       abort-on-fail: True
   - test:
      name: ceph ansible install rhcs 2 stable
      module: test_ansible.py
      config:
        use_cdn: True
        build: '2.0'
        ansi_config:
            ceph_test: True
            ceph_rhcs: True
            ceph_rhcs_cdn_install: True
            ceph_stable_release: jewel
            journal_collocation: True
            osd_auto_discovery: False
            journal_size: 1024
            ceph_stable: True
            ceph_stable_rh_storage: True
            public_network: 172.16.0.0/12
            fetch_directory: ~/fetch
            copy_admin_key: True
            ceph_conf_overrides:
                global:
                  osd_pool_default_pg_num: 128
                  osd_default_pool_size: 2
                  osd_pool_default_pgp_num: 128
                  mon_max_pg_per_osd: 4096
                  mon_allow_pool_delete: True
      desc: test cluster setup using ceph-ansible
      destroy-cluster: False
      abort-on-fail: True
   - test:
       name: ceph ansible upgrade to rhcs 3 nightly
       polarion-id: CEPH-83571475
       module: test_ansible_upgrade.py
       destroy-cluster: True
       config:
          build: '3.x'
          ansi_config:
            ceph_test: True
            ceph_origin: distro
            ceph_repository: rhcs
            ceph_rhcs_version: 3
            ceph_stable_release: luminous
            osd_scenario: collocated
            osd_auto_discovery: True
            upgrade_ceph_packages: True
            public_network: 172.16.0.0/12
            fetch_directory: ~/fetch
            copy_admin_key: True
            ceph_conf_overrides:
                global:
                  osd_pool_default_pg_num: 128
                  osd_default_pool_size: 2
                  osd_pool_default_pgp_num: 128
                  mon_max_pg_per_osd: 4096
                  mon_allow_pool_delete: True
       desc: Test Ceph-Ansible rolling update - 2.x -> 3.x
       destroy-cluster: False
       abort-on-fail: True
   - test:
      name: librbd workunit
      module: test_workunit.py
      config:
         test_name: rbd/test_librbd_python.sh
         branch: luminous
         role: mon
      desc: Test librbd unit tests
   - test:
      name: rbd cli automation
      module: rbd_system.py
      config:
         test_name: rbd_cli_automation.py
         branch: master
      desc: Test rbd cli automation tests
   - test:
      name: check-ceph-health
      module: exec.py
      config:
         cmd: ceph -s
         sudo: True
      desc: Check for ceph health debug info
   - test:
      name: rados_bench_test
      module: radosbench.py
      config:
         pg_num: '128'
         pool_type: 'normal'
      desc: run rados bench for 360 - normal profile
   - test:
      name: s3 tests
      module: test_s3.py
      config:
         branch: ceph-luminous
      desc: execution of s3 tests against radosgw
      destroy-cluster: False
   - test:
      name: ceph ansible purge
      module: purge_cluster.py
      desc: Purge ceph cluster
      abort-on-fail: True
      recreate-cluster: True

   - test:
      name: install ceph pre-requisities
      module: install_prereq.py
      abort-on-fail: True
   - test:
      name: ceph ansible install rhcs 2 stable
      module: test_ansible.py
      config:
        use_cdn: True
        build: '2.0'
        ansi_config:
            ceph_test: True
            ceph_rhcs: True
            ceph_rhcs_cdn_install: True
            ceph_stable_release: jewel
            journal_collocation: True
            osd_auto_discovery: True
            journal_size: 1024
            ceph_stable: True
            ceph_stable_rh_storage: True
            public_network: 172.16.0.0/12
            fetch_directory: ~/fetch
            copy_admin_key: True
            ceph_conf_overrides:
                global:
                  osd_pool_default_pg_num: 128
                  osd_default_pool_size: 2
                  osd_pool_default_pgp_num: 128
                  mon_max_pg_per_osd: 4096
                  mon_allow_pool_delete: True
      desc: test cluster setup using ceph-ansible
      destroy-cluster: False
      abort-on-fail: True
   - test:
       name: ceph ansible upgrade to rhcs 3 stable
       module: test_ansible_upgrade.py
       destroy-cluster: True
       config:
          use_cdn: True
          build: '3.0'
          ansi_config:
            ceph_test: True
            ceph_origin: repository
            ceph_repository: rhcs
            ceph_repository_type: cdn
            ceph_rhcs_version: 3
            ceph_stable_release: luminous
            ceph_repository: rhcs
            upgrade_ceph_packages: True
            radosgw_interface: eth0
            public_network: 172.16.0.0/12
            osd_scenario: collocated
            osd_auto_discovery: True
            fetch_directory: ~/fetch
            copy_admin_key: True
            ceph_conf_overrides:
                global:
                  osd_pool_default_pg_num: 128
                  osd_default_pool_size: 2
                  osd_pool_default_pgp_num: 128
                  mon_max_pg_per_osd: 4096
                  mon_allow_pool_delete: True
       desc: test rolling upgrade using ceph-ansible
       destroy-cluster: False
       abort-on-fail: True
   - test:
       name: ceph ansible upgrade to rhcs 3 nightly
       polarion-id: CEPH-83571476
       module: test_ansible_upgrade.py
       config:
          ansi_config:
            ceph_test: True
            ceph_origin: distro
            ceph_stable_release: luminous
            ceph_repository: rhcs
            upgrade_ceph_packages: True
            radosgw_interface: eth0
            public_network: 172.16.0.0/12
            osd_scenario: collocated
            osd_auto_discovery: True
            fetch_directory: ~/fetch
            copy_admin_key: True
       desc: Test Ceph-Ansible rolling update - 2.x -> 3.0 - 3.x
       abort-on-fail: True
   - test:
      name: librbd workunit
      module: test_workunit.py
      config:
         test_name: rbd/test_librbd_python.sh
         branch: luminous
         role: mon
      desc: Test librbd unit tests
   - test:
      name: rbd cli automation
      module: rbd_system.py
      config:
         test_name: rbd_cli_automation.py
         branch: master
      desc: Test rbd cli automation tests
   - test:
      name: check-ceph-health
      module: exec.py
      config:
         cmd: ceph -s
         sudo: True
      desc: Check for ceph health debug info
   - test:
      name: rados_bench_test
      module: radosbench.py
      config:
         pg_num: '128'
         pool_type: 'normal'
      desc: run rados bench for 360 - normal profile
   - test:
      name: s3 tests
      module: test_s3.py
      config:
         branch: ceph-luminous
      desc: execution of s3 tests against radosgw
      destroy-cluster: False
   - test:
      name: ceph ansible purge
      module: purge_cluster.py
      desc: Purge ceph cluster
      abort-on-fail: True
      recreate-cluster: True

   - test:
      name: install ceph pre-requisities
      module: install_prereq.py
      abort-on-fail: True
   - test:
      name: ceph ansible install rhcs 3 stable
      module: test_ansible.py
      config:
        use_cdn: True
        build: '3.0'
        ansi_config:
            ceph_test: True
            ceph_origin: repository
            ceph_repository: rhcs
            ceph_repository_type: cdn
            ceph_rhcs_version: 3
            ceph_stable_release: luminous
            osd_scenario: collocated
            osd_auto_discovery: False
            journal_size: 1024
            ceph_stable: True
            ceph_stable_rh_storage: True
            public_network: 172.16.0.0/12
            fetch_directory: ~/fetch
            copy_admin_key: True
            ceph_conf_overrides:
                global:
                  osd_pool_default_pg_num: 128
                  osd_default_pool_size: 2
                  osd_pool_default_pgp_num: 128
                  mon_max_pg_per_osd: 4096
                  mon_allow_pool_delete: True
      desc: test cluster setup using ceph-ansible
      destroy-cluster: False
      abort-on-fail: True
   - test:
       name: ceph ansible upgrade to rhcs 3 nightly
       polarion-id: CEPH-83571477
       module: test_ansible_upgrade.py
       config:
          ansi_config:
            ceph_test: True
            ceph_origin: distro
            ceph_stable_release: luminous
            ceph_repository: rhcs
            osd_scenario: collocated
            osd_auto_discovery: True
            upgrade_ceph_packages: True
            public_network: 172.16.0.0/12
            fetch_directory: ~/fetch
            copy_admin_key: True
            ceph_conf_overrides:
                global:
                  osd_pool_default_pg_num: 128
                  osd_default_pool_size: 2
                  osd_pool_default_pgp_num: 128
                  mon_max_pg_per_osd: 4096
                  mon_allow_pool_delete: True
       desc: Ceph-Ansible rolling update - 3.0 -> 3.x
       abort-on-fail: True
   - test:
      name: librbd workunit
      module: test_workunit.py
      config:
         test_name: rbd/test_librbd_python.sh
         branch: luminous
         role: mon
      desc: Test librbd unit tests
   - test:
      name: rbd cli automation
      module: rbd_system.py
      config:
         test_name: rbd_cli_automation.py
         branch: master
      desc: Test rbd cli automation tests
   - test:
      name: check-ceph-health
      module: exec.py
      config:
         cmd: ceph -s
         sudo: True
      desc: Check for ceph health debug info
   - test:
      name: rados_bench_test
      module: radosbench.py
      config:
         pg_num: '128'
         pool_type: 'normal'
      desc: run rados bench for 360 - normal profile
   - test:
      name: s3 tests
      module: test_s3.py
      config:
         branch: ceph-luminous
      desc: execution of s3 tests against radosgw
      destroy-cluster: False
   - test:
      name: ceph ansible purge
      module: purge_cluster.py
      desc: Purge ceph cluster
      destroy-cluster: True
