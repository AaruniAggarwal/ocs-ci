tests:
   - test:
       name: install ceph pre-requisities
       module: install_prereq.py
       abort-on-fail: true
   - test:
      name: ceph ansible
      module: test_ansible.py
      config:
        ansi_config:
            ceph_test: True
            ceph_origin: distro
            ceph_stable_release: luminous
            ceph_repository: rhcs
            osd_scenario: collocated
            osd_auto_discovery: False
            journal_size: 1024
            ceph_stable: True
            ceph_stable_rh_storage: True
            public_network: 172.16.0.0/12
            fetch_directory: ~/fetch
            copy_admin_key: true
            ceph_conf_overrides:
                global:
                  osd_pool_default_pg_num: 64
                  osd_default_pool_size: 2
                  osd_pool_default_pgp_num: 64
                  mon_max_pg_per_osd: 1024
                mon:
                  mon_allow_pool_delete: true
                  debug mon: 20
                mds:
                  mds_bal_split_size: 100
                  mds_bal_merge_size: 5
                  mds_bal_fragment_size_max: 10000
                  debug mds: 20

      desc: test cluster setup using ceph-ansible
      destroy-cluster: False
      abort-on-fail: true
   - test:
       name: multiple clients read/writing io
       module: CEPH-10528_10529.py
       polarion-id: CEPH-10528,CEPH-10529
       desc: Testing Single CephFS on multiple clients,performing IOs and checking file locking mechanism
       abort-on-fail: false
   - test:
       name: cephfs_mds_failover
       module: CEPH-11228.py
       config:
             num_of_dirs: 1000
       polarion-id: CEPH-11228
       desc: Testing MDSfailover on active-active mdss,performing client IOs with max:min directory pinning with 2 active mdss
       abort-on-fail: false
   - test:
       name: cephfs_mds_failover
       module: CEPH-11229.py
       config:
             num_of_dirs: 1000
       polarion-id: CEPH-11229
       desc: Testing MDSfailover on active-active mdss,performing client IOs with 10:2 directory pinning with 2 active mdss
       abort-on-fail: false
   - test:
       name: cephfs_mds_failover
       module: CEPH-11230.py
       config:
             num_of_dirs: 1000
       polarion-id: CEPH-11230
       desc: Testing MDSfailover on active-active mdss,performing client IOs with equal directory pinning with 2 active mdss
       abort-on-fail: false
   - test:
       name: cephfs_mds_failover
       module: CEPH-11232_11233.py
       polarion-id: CEPH-11232,11233
       desc: Testing MDSfailover on active-active mdss,performing client IOs with equal directory pinning with 2 active mdss
       abort-on-fail: false
   - test:
       name: cephfs_mds_failover
       module: CEPH-11242.py
       config:
             num_of_dirs: 1000
       polarion-id: CEPH-11242
       desc: Testing fs utilites along with mds failover on active-active mdss
       abort-on-fail: false
   - test:
       name: cephfs_mds_failover
       module: CEPH-11219.py
       polarion-id: CEPH-11219
       desc: Mount CephFS on multiple clients perform read and write operation on same file from different clients and perform delete ops
       abort-on-fail: false
   - test:
       name: cephfs_io
       module: CEPH-11220.py
       polarion-id: CEPH-11220
       desc: Mount CephFS on multiple clients, On same directory perform write from some clients and do read that data from some clients
       abort-on-fail: false
   - test:
       name: cephfs_io
       module: CEPH-11222.py
       polarion-id: CEPH-11222
       desc: Mount CephFS on multiple clients and run Stress IO from all clients on different directories,renaming a directory
       abort-on-fail: false
   - test:
       name: cephfs_rsync
       module: CEPH-11298.py
       polarion-id: CEPH-11298
       desc: Mount CephFS on multiple clients and rsync files and directories to outside the filesystem and vice-versa
       abort-on-fail: false
   - test:
       name: cephfs_file_layouts
       module: CEPH-11334.py
       polarion-id: CEPH-11334
       desc: Mount CephFS on multiple clients and perform file layout operations
       abort-on-fail: false
   - test:
       name: cephfs_basics
       module: cephfs_basic_tests.py
       polarion-id: CEPH-11293,CEPH-11296,CEPH-11297,CEPH-11295
       desc: Mount CephFS on multiple clients, On same directory perform write from some clients and do read that data from some clients
       abort-on-fail: false
   - test:
       name: cephfs_file_layouts
       module: CEPH-11335.py
       polarion-id: CEPH-11335
       desc: Mount CephFS on multiple clients and perform client eviction
       abort-on-fail: false